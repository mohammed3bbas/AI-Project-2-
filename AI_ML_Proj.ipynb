{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-ML-Proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Z6GpftsYF9"
      },
      "source": [
        "# Import Libraries "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Unb7QcvsfNc"
      },
      "source": [
        "first , we are going to import the required libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbpAqf92swUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de173682-4fcc-4fa3-be0e-0284cad70c01"
      },
      "source": [
        "import pandas as pd\r\n",
        "from pandas import DataFrame\r\n",
        "import sklearn\r\n",
        "import nltk\r\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer \r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from nltk.corpus import opinion_lexicon\r\n",
        "from sklearn import tree\r\n",
        "import numpy as np\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn import tree\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('opinion_lexicon')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWWtMdSxvSWI"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVC3YZctLS0"
      },
      "source": [
        "after that, we read the CSV file using pandas library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McJlsbsOtUJX"
      },
      "source": [
        "data = pd.read_csv(\"Output.csv\", low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sSPKmH7zAMQ",
        "outputId": "48bc09b8-4a72-49ec-9235-bc477b0f5718"
      },
      "source": [
        "data.Filtered.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    95698\n",
              "1.0    19763\n",
              "Name: Filtered, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtWLmPotZxbZ"
      },
      "source": [
        "data = data.head(70000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_y7OZCLdeca"
      },
      "source": [
        "handle not a NaN values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTQws0zfaayB"
      },
      "source": [
        "#reviewerID\treviewContent\trating\tusefulCount\tdate\treviewerID.1\tfirstCount\treviewCount\tFiltered\r\n",
        "data['reviewID'] = data['reviewID'].fillna(\"-1\")\r\n",
        "data['reviewerID'] = data['reviewerID'].fillna(\"-1\")\r\n",
        "data['reviewContent'] = data['reviewContent'].fillna(\"-1\")\r\n",
        "data['rating'] = data['rating'].fillna(-1)\r\n",
        "data['usefulCount'] = data['usefulCount'].fillna(-1)\r\n",
        "data['date'] = data['date'].fillna(\"-1\")\r\n",
        "data['firstCount'] = data['firstCount'].fillna(-1)\r\n",
        "data['reviewCount'] = data['reviewCount'].fillna(-1)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbVK58GaosRK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFAZc2a4b2JR"
      },
      "source": [
        "here we toknize each review content and put each token [list of words] in a list "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJt_g-6jVwTk"
      },
      "source": [
        "list_of_tokens =[]\r\n",
        "for column in data['reviewContent']:\r\n",
        "   text_tokens = word_tokenize(column)\r\n",
        "   list_of_tokens.append(text_tokens)\r\n",
        "len(list_of_tokens)\r\n",
        "list_of_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l04kPbs6ck5"
      },
      "source": [
        "remove stop words and lemmatization on review contents "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ_z7oLh6hND"
      },
      "source": [
        "def stopWordRemove(list_of_tokens):\r\n",
        "  tokens_without_sw=[]\r\n",
        "  sw = set(stopwords.words('english'))\r\n",
        "  for i in range(0,len(list_of_tokens)):\r\n",
        "    words_list = []\r\n",
        "    for word in list_of_tokens[i]:\r\n",
        "      # print(word)\r\n",
        "      if not (word in sw):\r\n",
        "        words_list.append(word)\r\n",
        "    tokens_without_sw.append(words_list)\r\n",
        "  return tokens_without_sw\r\n",
        "tokens_without_sw = stopWordRemove(list_of_tokens)\r\n",
        "tokens_without_sw\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-rZtHc3mD6O"
      },
      "source": [
        "now we are going to take the output of the *stopWordRemove* and lemmatize the words on it by using the function *lemmatization* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPxcHKDTl8F7"
      },
      "source": [
        "def lemmatization(tokens_without_sw):\r\n",
        "  lmtz_list=[]\r\n",
        "  lemmatizer = WordNetLemmatizer()\r\n",
        "  for words_list in tokens_without_sw:\r\n",
        "    lmtz_words_list = []\r\n",
        "    for word in words_list:\r\n",
        "      lmtz_words_list.append(lemmatizer.lemmatize(word))\r\n",
        "    lmtz_list.append(lmtz_words_list)\r\n",
        "  return  lmtz_list\r\n",
        "lmtz_list=lemmatization(tokens_without_sw)\r\n",
        "lmtz_list #without sw and lemmatized "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzJWdudu6mPS"
      },
      "source": [
        "# Feature Extraction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HArtaMTIWND"
      },
      "source": [
        "now we are going to use Bag of words to train our model \r\n",
        "first , we are going to concat the sentances in the *lmtz_list* (stop word removed and lemmatized list) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_70wlnP1NcoA"
      },
      "source": [
        "corpus = []\r\n",
        "#join the words to use it in vectorizer \r\n",
        "\r\n",
        "for list_of_words in lmtz_list:\r\n",
        "  corpus.append((\" \").join(list_of_words))\r\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kza8qB4g3JOa"
      },
      "source": [
        "after that, we will use *CountVectorizer* to create the bag of words array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJOXjIB14eaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ef0ca2-fb71-45ac-8727-64d373de0483"
      },
      "source": [
        "vectorizer = CountVectorizer(max_df=0.90, min_df=0.10)\r\n",
        "X = vectorizer.fit_transform(corpus)\r\n",
        "print(vectorizer.get_feature_names())\r\n",
        "X = X.toarray()\r\n",
        "# print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['also', 'always', 'area', 'back', 'bar', 'best', 'come', 'could', 'day', 'even', 'first', 'food', 'friendly', 'get', 'go', 'good', 'got', 'great', 'it', 'know', 'like', 'little', 'lot', 'love', 'make', 'much', 'my', 'nice', 'night', 'one', 'people', 'place', 'pretty', 'price', 'really', 'restaurant', 'right', 'service', 'staff', 'the', 'there', 'they', 'thing', 'this', 'time', 've', 'way', 'we', 'well', 'went', 'would']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ebht3ZDFb5"
      },
      "source": [
        "**SENTIMENT ANALYSIS TO CLASSIFY EXAGGERATEDOR REVIEWS OR \" TOO POSITIVE\" AND \" TOO NEGATIVE\" REVIEWS**\r\n",
        "this done by calculating avg poliratity score of each word in a sentance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-mKRaZYDG5T"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\r\n",
        "\r\n",
        "def exaggeratedReviews(lmtz_list):\r\n",
        "    sid = SentimentIntensityAnalyzer()\r\n",
        "    exaggerated_list =[]\r\n",
        "    weight = 0\r\n",
        "\r\n",
        "    for sentance in lmtz_list:\r\n",
        "      weight =0 \r\n",
        "      for word in sentance: \r\n",
        "        weight = weight+sid.polarity_scores(word)['compound']\r\n",
        "      exaggerated_list.append(abs(weight/len(sentance))) #avg weight \r\n",
        "    return exaggerated_list\r\n",
        "\r\n",
        "exaggerated_list = exaggeratedReviews(lmtz_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZpaCxAPbiwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45a9a4a-b562-4b99-9d45-c09bcc1c8e63"
      },
      "source": [
        "# len(exaggerated_list)\r\n",
        "max(exaggerated_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NimqkNerVm3u"
      },
      "source": [
        "here we will divide the senances into 3 subsets \r\n",
        "\r\n",
        "1.   Too exaggerated \r\n",
        "2.   regular \r\n",
        "3.   not exaggerated \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "by using a threshold which is the mean value +- standard diviation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipwpboP4VmWC",
        "outputId": "68223bf6-061f-4941-bcd5-0563a18ecaf3"
      },
      "source": [
        "exDF = pd.DataFrame(exaggerated_list)\r\n",
        "mean = exDF.mean(axis = 0, skipna = True) \r\n",
        "mean[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.036857705947135805"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXFrl720XRG5",
        "outputId": "3f95db30-de7a-428b-b571-c46d06dce2ba"
      },
      "source": [
        "std = exDF.std(axis = 0, skipna = True)\r\n",
        "std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.028297\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "7dvR4oUGXsTR",
        "outputId": "a16b60a5-ceed-4c63-ac74-87e49608854e"
      },
      "source": [
        "# exDF \r\n",
        "for i in range(0,len(exaggerated_list)):\r\n",
        "  if exaggerated_list[i] > (mean[0]+std[0]):\r\n",
        "    exaggerated_list[i]= 1\r\n",
        "  elif exaggerated_list[i]< (mean[0]-std[0]):\r\n",
        "    exaggerated_list[i]=-1\r\n",
        "  else:\r\n",
        "    exaggerated_list[i]=0 \r\n",
        "# exaggerated_list\r\n",
        "pd.DataFrame(exaggerated_list)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0\n",
              "0      0\n",
              "1     -1\n",
              "2      0\n",
              "3      0\n",
              "4     -1\n",
              "...   ..\n",
              "69995  0\n",
              "69996  0\n",
              "69997  0\n",
              "69998 -1\n",
              "69999  0\n",
              "\n",
              "[70000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATIEl7TxiJWR",
        "outputId": "22b83456-f5b2-4f1d-9234-70115d778fc2"
      },
      "source": [
        "print(max(exaggerated_list),min(exaggerated_list),0 in exaggerated_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -1 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr_xYW8W9QcC"
      },
      "source": [
        "*review content* that contains many symbols such as \"<3 < 3 \" are often useless and considered spam.\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W35MOlLv9sQz"
      },
      "source": [
        "\r\n",
        "def extractExclamationQuestionMarks(lmtz_list):\r\n",
        "\r\n",
        "  count = 0;  \r\n",
        "  numofexclamation = []\r\n",
        "  marks = ['!', \">\" ,\"\\'\" ,\";\" ,\"\\\"\", \"-\" ,\"?\",\"<\"]\r\n",
        "  for listt in lmtz_list: \r\n",
        "      count=0\r\n",
        "      for word in listt:   \r\n",
        "         #Checks whether given character is a punctuation mark  \r\n",
        "         if word in marks: \r\n",
        "            count = count + 1  \r\n",
        "      numofexclamation.append(count)\r\n",
        "  return(numofexclamation)\r\n",
        "numofexclamation=extractExclamationQuestionMarks(lmtz_list)\r\n",
        "numofexclamation\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvcbh8kuaq2i"
      },
      "source": [
        "here we will do cosin similarty , and calculate the avg similarty for each row\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "(can't be run on a data more than 20000) !! ram crashed \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS3ujQtQ0c16"
      },
      "source": [
        "doc_term_matrix = X.todense()\r\n",
        "df = pd.DataFrame(doc_term_matrix, \r\n",
        "                  columns=vectorizer.get_feature_names())\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6hondga2Fkh"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "cosSim=cosine_similarity(df, df)\r\n",
        "cosSimDF['mean'] = (pd.DataFrame(cosSim)).mean(axis = 1, skipna = True) \r\n",
        "# calculate the mean of each row \r\n",
        "# cosSimDF['mean'] = cosSimDF.mean(axis = 1, skipna = True) \r\n",
        "cosSimDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsfS0dqRLRUB"
      },
      "source": [
        "cosSimAVG = cosSimDF['mean'].tolist()\r\n",
        "cosSimAVG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s08aDKG0i4sd"
      },
      "source": [
        "max(cosSimAVG)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0VNfkxAB7yb"
      },
      "source": [
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "first, we create new Dataframe for *feature extraction* features with *reviewContents* and *Filtered* columns\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFtm38XvB4aR"
      },
      "source": [
        "featureData = data[ ['reviewContent','Filtered'] ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMD4ciMNBdjW"
      },
      "source": [
        "here, we are going to calculate the length of the **review content** using *reviewContentLength* method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jin1gjGbBj3J"
      },
      "source": [
        "def reviewContentLength(str):\r\n",
        "    return len(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8p_2nH8CO2N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "f8df8fe7-d500-4e0b-cf47-6a3cb0256bf5"
      },
      "source": [
        "featureData['length'] = data.apply( lambda row: reviewContentLength(row['reviewContent']), axis = 1 )\r\n",
        "featureData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewContent</th>\n",
              "      <th>Filtered</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I came to Chicago on business and was initiall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I still love this place but the took Mu Shu of...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alan is the best CPA that I know. He knows all...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My family has been using Korean Air for many m...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I hadn't been to Grisanti's since a pre-homeco...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>I'm pretty sure a record skipped as soon as we...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>An admitted Parrot Head I couldn't not go to M...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>Another failed business.   Wasn't terribly sur...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>I have visited this location twice -- today an...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>This location is wedged inside the mall and is...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           reviewContent  Filtered  length\n",
              "0      I came to Chicago on business and was initiall...       0.0     767\n",
              "1      I still love this place but the took Mu Shu of...       0.0     101\n",
              "2      Alan is the best CPA that I know. He knows all...       0.0     466\n",
              "3      My family has been using Korean Air for many m...       0.0     791\n",
              "4      I hadn't been to Grisanti's since a pre-homeco...       0.0     795\n",
              "...                                                  ...       ...     ...\n",
              "69995  I'm pretty sure a record skipped as soon as we...       0.0     794\n",
              "69996  An admitted Parrot Head I couldn't not go to M...       1.0     399\n",
              "69997  Another failed business.   Wasn't terribly sur...       0.0     551\n",
              "69998  I have visited this location twice -- today an...       0.0     313\n",
              "69999  This location is wedged inside the mall and is...       0.0     260\n",
              "\n",
              "[70000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai32Gc_gl9NL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ff4b65-b86e-4176-87d7-9f75d6af23fc"
      },
      "source": [
        "contentLengthList = featureData['length'].to_list()\r\n",
        "contentLengthList\r\n",
        "len(contentLengthList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMmXs6ycZUa"
      },
      "source": [
        "now we catogrize lenghth to \r\n",
        "\r\n",
        "1.  so tall 1 : when > avg+std \r\n",
        "2.  too short -1 : when < avg+std \r\n",
        "3.  noraml : other wise \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ko07iacv7c",
        "outputId": "38e2adf7-a54a-4f5f-f44d-941579af1074"
      },
      "source": [
        "contentLengthListDF = pd.DataFrame(contentLengthList)\r\n",
        "mean = contentLengthListDF.mean(axis = 0, skipna = True) \r\n",
        "mean[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "546.1968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kAlaZssc8Mp",
        "outputId": "9dc1c829-33e1-436b-fbd1-8296e8ca67ed"
      },
      "source": [
        "std = contentLengthListDF.std(axis = 0, skipna = True)\r\n",
        "std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    243.160685\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Bh15kUavdCfr",
        "outputId": "d3115031-a1b7-4d34-e358-7b08c90bcc15"
      },
      "source": [
        "for i in range(0,len(contentLengthList)):\r\n",
        "  if contentLengthList[i] > (mean[0]+std[0]):\r\n",
        "    contentLengthList[i]= 1\r\n",
        "  elif contentLengthList[i]< (mean[0]-std[0]):\r\n",
        "    contentLengthList[i]=-1\r\n",
        "  else:\r\n",
        "    contentLengthList[i]=0 \r\n",
        "pd.DataFrame(contentLengthList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69995</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69996</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69997</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69998</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69999</th>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0\n",
              "0      0\n",
              "1     -1\n",
              "2      0\n",
              "3      1\n",
              "4      1\n",
              "...   ..\n",
              "69995  1\n",
              "69996  0\n",
              "69997  0\n",
              "69998  0\n",
              "69999 -1\n",
              "\n",
              "[70000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ3EyDTidJS7",
        "outputId": "9d45cafa-581c-4203-dbf7-7dc29460422c"
      },
      "source": [
        "print(max(contentLengthList),min(contentLengthList),0 in contentLengthList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -1 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxIssccnPu9U"
      },
      "source": [
        "also, we will calculate the **number of times the personal pronouns appears** using *personalPronounsCount* method "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XVKdNQBP__o"
      },
      "source": [
        "def personalPronounsCount(lmtz_list):\r\n",
        "  '''This function finds the number of personal pronounns like I,Me,myself,mine etc. in the review text'''\r\n",
        "  # tokens = nltk.word_tokenize(str)\r\n",
        "  pl = []\r\n",
        "  for sentance in lmtz_list:\r\n",
        "    count=0\r\n",
        "    taged = nltk.pos_tag(sentance)\r\n",
        "    for i in taged:\r\n",
        "      if i[1] == 'PRP' or i[1] == 'PRP$':\r\n",
        "        count=count+1\r\n",
        "    pl.append(count)\r\n",
        "  return pl\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BnDRNgSVfMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b047095e-1d17-4d6e-ce1a-402c8810bed1"
      },
      "source": [
        "pl =personalPronounsCount(lmtz_list)\r\n",
        "pl\r\n",
        "# featureData"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4,\n",
              " 2,\n",
              " 8,\n",
              " 3,\n",
              " 10,\n",
              " 8,\n",
              " 0,\n",
              " 8,\n",
              " 3,\n",
              " 5,\n",
              " 8,\n",
              " 1,\n",
              " 6,\n",
              " 8,\n",
              " 0,\n",
              " 10,\n",
              " 3,\n",
              " 5,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 9,\n",
              " 2,\n",
              " 7,\n",
              " 0,\n",
              " 9,\n",
              " 2,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 10,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 11,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 5,\n",
              " 7,\n",
              " 0,\n",
              " 7,\n",
              " 6,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 4,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 2,\n",
              " 11,\n",
              " 0,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 7,\n",
              " 0,\n",
              " 14,\n",
              " 4,\n",
              " 3,\n",
              " 8,\n",
              " 11,\n",
              " 6,\n",
              " 7,\n",
              " 1,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 9,\n",
              " 10,\n",
              " 4,\n",
              " 9,\n",
              " 4,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 8,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 11,\n",
              " 13,\n",
              " 5,\n",
              " 3,\n",
              " 0,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 9,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 5,\n",
              " 0,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 12,\n",
              " 5,\n",
              " 10,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 5,\n",
              " 5,\n",
              " 0,\n",
              " 3,\n",
              " 7,\n",
              " 3,\n",
              " 7,\n",
              " 1,\n",
              " 12,\n",
              " 9,\n",
              " 4,\n",
              " 1,\n",
              " 5,\n",
              " 5,\n",
              " 7,\n",
              " 3,\n",
              " 15,\n",
              " 6,\n",
              " 15,\n",
              " 6,\n",
              " 9,\n",
              " 10,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 9,\n",
              " 5,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 9,\n",
              " 2,\n",
              " 9,\n",
              " 13,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 3,\n",
              " 12,\n",
              " 2,\n",
              " 2,\n",
              " 10,\n",
              " 10,\n",
              " 7,\n",
              " 0,\n",
              " 7,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 5,\n",
              " 8,\n",
              " 3,\n",
              " 3,\n",
              " 10,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 4,\n",
              " 12,\n",
              " 4,\n",
              " 2,\n",
              " 10,\n",
              " 7,\n",
              " 3,\n",
              " 6,\n",
              " 13,\n",
              " 1,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 4,\n",
              " 3,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 10,\n",
              " 2,\n",
              " 9,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 8,\n",
              " 6,\n",
              " 9,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 0,\n",
              " 7,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 8,\n",
              " 1,\n",
              " 11,\n",
              " 10,\n",
              " 2,\n",
              " 3,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 6,\n",
              " 1,\n",
              " 11,\n",
              " 3,\n",
              " 2,\n",
              " 7,\n",
              " 3,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 5,\n",
              " 3,\n",
              " 4,\n",
              " 7,\n",
              " 7,\n",
              " 0,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 10,\n",
              " 8,\n",
              " 8,\n",
              " 6,\n",
              " 1,\n",
              " 9,\n",
              " 15,\n",
              " 6,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 10,\n",
              " 2,\n",
              " 5,\n",
              " 9,\n",
              " 4,\n",
              " 3,\n",
              " 4,\n",
              " 8,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 13,\n",
              " 10,\n",
              " 5,\n",
              " 7,\n",
              " 8,\n",
              " 0,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 6,\n",
              " 8,\n",
              " 5,\n",
              " 3,\n",
              " 0,\n",
              " 3,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 2,\n",
              " 10,\n",
              " 2,\n",
              " 6,\n",
              " 0,\n",
              " 4,\n",
              " 11,\n",
              " 6,\n",
              " 2,\n",
              " 4,\n",
              " 7,\n",
              " 6,\n",
              " 0,\n",
              " 14,\n",
              " 3,\n",
              " 2,\n",
              " 12,\n",
              " 4,\n",
              " 6,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 3,\n",
              " 5,\n",
              " 6,\n",
              " 0,\n",
              " 5,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 8,\n",
              " 0,\n",
              " 12,\n",
              " 3,\n",
              " 3,\n",
              " 9,\n",
              " 4,\n",
              " 6,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 6,\n",
              " 7,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 3,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 16,\n",
              " 1,\n",
              " 5,\n",
              " 8,\n",
              " 7,\n",
              " 5,\n",
              " 8,\n",
              " 7,\n",
              " 5,\n",
              " 0,\n",
              " 5,\n",
              " 1,\n",
              " 6,\n",
              " 7,\n",
              " 0,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 0,\n",
              " 7,\n",
              " 4,\n",
              " 8,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 9,\n",
              " 7,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 7,\n",
              " 2,\n",
              " 3,\n",
              " 7,\n",
              " 10,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 10,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 9,\n",
              " 3,\n",
              " 8,\n",
              " 0,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 10,\n",
              " 5,\n",
              " 3,\n",
              " 1,\n",
              " 10,\n",
              " 3,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 0,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 9,\n",
              " 12,\n",
              " 4,\n",
              " 3,\n",
              " 7,\n",
              " 0,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 0,\n",
              " 7,\n",
              " 5,\n",
              " 0,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 5,\n",
              " 5,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 8,\n",
              " 8,\n",
              " 4,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 9,\n",
              " 5,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 4,\n",
              " 0,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 7,\n",
              " 4,\n",
              " 4,\n",
              " 13,\n",
              " 17,\n",
              " 3,\n",
              " 4,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 0,\n",
              " 6,\n",
              " 1,\n",
              " 4,\n",
              " 11,\n",
              " 5,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 4,\n",
              " 4,\n",
              " 3,\n",
              " 8,\n",
              " 4,\n",
              " 7,\n",
              " 3,\n",
              " 2,\n",
              " 10,\n",
              " 9,\n",
              " 2,\n",
              " 0,\n",
              " 12,\n",
              " 5,\n",
              " 0,\n",
              " 6,\n",
              " 8,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 10,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 7,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 10,\n",
              " 10,\n",
              " 0,\n",
              " 1,\n",
              " 7,\n",
              " 11,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 4,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 7,\n",
              " 10,\n",
              " 2,\n",
              " 4,\n",
              " 3,\n",
              " 8,\n",
              " 4,\n",
              " 0,\n",
              " 2,\n",
              " 15,\n",
              " 8,\n",
              " 0,\n",
              " 5,\n",
              " 4,\n",
              " 6,\n",
              " 7,\n",
              " 4,\n",
              " 5,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 12,\n",
              " 4,\n",
              " 7,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 7,\n",
              " 6,\n",
              " 12,\n",
              " 2,\n",
              " 10,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 0,\n",
              " 6,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 2,\n",
              " 4,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 6,\n",
              " 1,\n",
              " 5,\n",
              " 11,\n",
              " 8,\n",
              " 2,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 5,\n",
              " 2,\n",
              " 8,\n",
              " 10,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 5,\n",
              " 3,\n",
              " 0,\n",
              " 5,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 4,\n",
              " 6,\n",
              " 4,\n",
              " 9,\n",
              " 3,\n",
              " 2,\n",
              " 8,\n",
              " 3,\n",
              " 5,\n",
              " 3,\n",
              " 5,\n",
              " 11,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 6,\n",
              " 6,\n",
              " 7,\n",
              " 16,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 8,\n",
              " 12,\n",
              " 10,\n",
              " 4,\n",
              " 0,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 7,\n",
              " 9,\n",
              " 2,\n",
              " 5,\n",
              " 5,\n",
              " 0,\n",
              " 4,\n",
              " 1,\n",
              " 3,\n",
              " 0,\n",
              " 12,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 4,\n",
              " 10,\n",
              " 4,\n",
              " 11,\n",
              " 12,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 0,\n",
              " 5,\n",
              " 4,\n",
              " 0,\n",
              " 2,\n",
              " 8,\n",
              " 2,\n",
              " 0,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 8,\n",
              " 4,\n",
              " 4,\n",
              " 4,\n",
              " 3,\n",
              " 6,\n",
              " 9,\n",
              " 0,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 7,\n",
              " 6,\n",
              " 7,\n",
              " 9,\n",
              " 7,\n",
              " 5,\n",
              " 0,\n",
              " 12,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 11,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 9,\n",
              " 9,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 1,\n",
              " 2,\n",
              " 9,\n",
              " 1,\n",
              " 10,\n",
              " 2,\n",
              " 4,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 11,\n",
              " 9,\n",
              " 3,\n",
              " 5,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 3,\n",
              " 2,\n",
              " 5,\n",
              " 4,\n",
              " 4,\n",
              " 0,\n",
              " 13,\n",
              " 9,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 2,\n",
              " 5,\n",
              " 4,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 7,\n",
              " 9,\n",
              " 3,\n",
              " 3,\n",
              " 11,\n",
              " 7,\n",
              " 5,\n",
              " 3,\n",
              " 0,\n",
              " 10,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 4,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 7,\n",
              " 3,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 11,\n",
              " 8,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 8,\n",
              " 5,\n",
              " 11,\n",
              " 7,\n",
              " 11,\n",
              " 10,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 7,\n",
              " 0,\n",
              " 6,\n",
              " 7,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 5,\n",
              " 11,\n",
              " 3,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 8,\n",
              " 10,\n",
              " 5,\n",
              " 7,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 9,\n",
              " 7,\n",
              " 4,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 7,\n",
              " 1,\n",
              " 10,\n",
              " 7,\n",
              " 15,\n",
              " 1,\n",
              " 5,\n",
              " 7,\n",
              " 0,\n",
              " 0,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 8,\n",
              " 5,\n",
              " 4,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 4,\n",
              " 7,\n",
              " 4,\n",
              " 6,\n",
              " 10,\n",
              " 0,\n",
              " 1,\n",
              " 3,\n",
              " 9,\n",
              " 4,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 7,\n",
              " 0,\n",
              " 5,\n",
              " 6,\n",
              " 8,\n",
              " 7,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 7,\n",
              " 10,\n",
              " 11,\n",
              " 3,\n",
              " 2,\n",
              " 8,\n",
              " 7,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 5,\n",
              " 4,\n",
              " 4,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnIHiTfcmbyl"
      },
      "source": [
        "personalPronounsCountList = pl\r\n",
        "personalPronounsCountList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGkBSNdHprQU",
        "outputId": "69f4b612-20f0-4bfa-e65d-b34a1e78f67d"
      },
      "source": [
        "len(personalPronounsCountList)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWR5AU7t6Y7P"
      },
      "source": [
        "# Classification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLMdQ3k-ntQE"
      },
      "source": [
        "before creating our models, we are going to arrange our fetures in a wanted data structure "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg5rroj4oAq3"
      },
      "source": [
        "import numpy as np\r\n",
        "X = np.array(X)\r\n",
        "# X = np.array(contentLengthList)\r\n",
        "X= np.c_[X, contentLengthList]\r\n",
        "X = np.c_[X, personalPronounsCountList]\r\n",
        "X = np.c_[X, data['rating']]\r\n",
        "X = np.c_[X, data['usefulCount']]\r\n",
        "X = np.c_[X, data['reviewCount']]\r\n",
        "X = np.c_[X, data['firstCount']]\r\n",
        "X = np.c_[X , numofexclamation]\r\n",
        "# x = np.c_[X , cosSimAVG]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# *************************************************************\r\n",
        "# X = np.array(X) # easier to deal with numpy arrays rather thna 2d list \r\n",
        "# X=np.c_[X, contentLengthList] #adding features colums \r\n",
        "# X=np.c_[X, personalPronounsCountList]\r\n",
        "# X = np.c_[X, data['rating']]\r\n",
        "# X = np.c_[X, data['usefulCount']]\r\n",
        "# X = np.c_[X, data['reviewCount']]\r\n",
        "# X = np.c_[X, data['firstCount']]\r\n",
        "# X = np.c_[X , numofexclamation]\r\n",
        "\r\n",
        "# X\r\n",
        "# X.size\r\n",
        "#**************************************************************\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC8FS40JobEj",
        "outputId": "c00f8b37-9cd5-4f3c-8211-ab1ca495f1eb"
      },
      "source": [
        "len(X) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmQmd9Mcz-7m"
      },
      "source": [
        "#Holdout set \r\n",
        "after that we will divide our dataset into 70% training 30% testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDWKy_c10GSo"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X= X.tolist()\r\n",
        "Y = data['Filtered'].tolist()\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.30 )\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2eC0bqt8oPk"
      },
      "source": [
        "Handle NaN data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt_Q_oMDtpy1"
      },
      "source": [
        "\r\n",
        "X_train = np.nan_to_num(np.array(X_train))\r\n",
        "X_test = np.nan_to_num(np.array(X_test))\r\n",
        "Y_train = np.nan_to_num(np.array(Y_train))\r\n",
        "Y_test = np.nan_to_num(np.array(Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYn1suj1-SWf"
      },
      "source": [
        "X_train=np.array(X_train).astype(np.float)\r\n",
        "X_test=np.array(X_test).astype(np.float)\r\n",
        "Y_train=np.array(Y_train).astype(np.float)\r\n",
        "Y_test=np.array(Y_test).astype(np.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Iql8bp6dAP"
      },
      "source": [
        "Decision Trees (DT) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt01OE856cjP"
      },
      "source": [
        "from sklearn import tree\r\n",
        "\r\n",
        "clf = tree.DecisionTreeClassifier()\r\n",
        "clf = clf.fit(X_train, Y_train)\r\n",
        "# clf.max_depth()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0Gx2L39WH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cda9fa-dee1-47b4-c249-51723a0ed2b6"
      },
      "source": [
        "\r\n",
        "count=0\r\n",
        "predicted_out=clf.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7715714285714286 0.35324255089658896 0.3352098259979529 0.3733257338273012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgIlO8xubEJH"
      },
      "source": [
        "Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tMWNiDJbJy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97b6a5a-1ea2-4084-f727-0a17616efae8"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "\r\n",
        "gnb = GaussianNB()\r\n",
        "gnb_model = gnb.fit(X_train, Y_train)\r\n",
        "gnb_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4jJTu646LiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e7bda2-57b1-4efe-c6e6-4d35f3268c43"
      },
      "source": [
        "count=0\r\n",
        "predicted_out=gnb_model.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))\r\n",
        "\r\n",
        "# for i in range (0,len(Y_test)):\r\n",
        "#    if predicted_out[i] == Y_test[i]:\r\n",
        "#           count+=1\r\n",
        "# print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6349047619047619 0.3226433430515063 0.23380281690140844 0.5203761755485894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_f-ECmOhxhv"
      },
      "source": [
        "Neural netweork multi layer perceptron "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbYExfWZh038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b944f7c-da99-4431-823d-9aa94362a982"
      },
      "source": [
        "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                    hidden_layer_sizes=(3, 3), random_state=1)\r\n",
        "\r\n",
        "mlp_model.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(3, 3), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5CysIiQ6USs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b172999-a1f7-48cb-c3f3-574c7f95a34b"
      },
      "source": [
        "count=0\r\n",
        "predicted_out=mlp_model.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.17704761904761904 0.27563081565931763 0.16158042164234115 0.9370190937589057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6NpPkj3tTL"
      },
      "source": [
        "# NOW WE ARE GOING TO BALNCE THE DATA !! using smote"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CPdvI1N3ySy",
        "outputId": "9f8daf45-95ca-478c-c28c-399f251103dc"
      },
      "source": [
        "from collections import Counter\r\n",
        "from imblearn.over_sampling import SMOTE\r\n",
        "\r\n",
        "SMOTE = SMOTE()\r\n",
        "X_train_SMOTE, Y_train_SMOTE = SMOTE.fit_resample(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDlMcGJ94Cgw",
        "outputId": "47443806-42a3-469c-e7f9-79ba4883f1e3"
      },
      "source": [
        "print(\"befor balncing: \"+str(Counter(Y_train)),\"after balancing: \"+ str(Counter(Y_train_SMOTE)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "befor balncing: Counter({0.0: 40622, 1.0: 8378}) after balancing: Counter({0.0: 40622, 1.0: 40622})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JauyOWae4jdW"
      },
      "source": [
        "now we will apply the same classifiers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D_slTOZ4qmd",
        "outputId": "1e2294fd-21cc-4be5-8e23-4066bca14160"
      },
      "source": [
        "from sklearn import tree\r\n",
        "\r\n",
        "DT = tree.DecisionTreeClassifier()\r\n",
        "DT = DT.fit(X_train_SMOTE, Y_train_SMOTE)\r\n",
        "gnb = GaussianNB()\r\n",
        "gnb_model = gnb.fit(X_train_SMOTE, Y_train_SMOTE)\r\n",
        "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5,\r\n",
        "                    hidden_layer_sizes=(3, 3), random_state=1)\r\n",
        "\r\n",
        "mlp_model=mlp_model.fit(X_train_SMOTE, Y_train_SMOTE)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knCSBUYq5HYb"
      },
      "source": [
        "now we will see the results "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STFlbecN5KHT",
        "outputId": "0079c3c7-d935-4c36-a4a8-8638335d1b44"
      },
      "source": [
        "DT_predicted=clf.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, DT_predicted),f1_score(Y_test, DT_predicted),precision_score(Y_test, DT_predicted), recall_score(Y_test, DT_predicted))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7715714285714286 0.35324255089658896 0.3352098259979529 0.3733257338273012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byQe5H2x5lT2",
        "outputId": "487f8c10-5ebe-46b1-ea71-5d081c128571"
      },
      "source": [
        "predicted_out=gnb_model.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5708571428571428 0.30063634952661805 0.206569265223419 0.5520091194072385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5z7mrfL5-CC",
        "outputId": "562aed47-4ab2-4c25-a5aa-9cef1201ca4c"
      },
      "source": [
        "predicted_out=mlp_model.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, predicted_out),f1_score(Y_test, predicted_out),precision_score(Y_test, predicted_out), recall_score(Y_test, predicted_out))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8500952380952381 0.3744038155802861 0.618516086671044 0.268452550584212\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}